import requests
import argparse
import json


def verificar_modelos_ollama():
    """
    Fun√ß√£o para verificar a conex√£o com a API do Ollama e listar os modelos dispon√≠veis.
    """
    try:
        # Fazer requisi√ß√£o GET para o endpoint da API do Ollama
        response = requests.get("http://localhost:11434/api/tags")
        
        # Verificar se a resposta teve status 200 (sucesso)
        if response.status_code == 200:
            # Extrair o JSON da resposta
            data = response.json()
            
            # Pegar a lista de modelos
            modelos = data.get('models', [])
            
            # Extrair apenas os nomes dos modelos para exibi√ß√£o mais limpa
            nomes_modelos = [modelo.get('name', 'Nome n√£o dispon√≠vel') for modelo in modelos]
            
            # Imprimir mensagem de sucesso com os modelos encontrados
            print(f"‚úÖ Conex√£o com Ollama bem-sucedida! Modelos dispon√≠veis: {nomes_modelos}")
            return True
            
        else:
            # Se o status for diferente de 200, imprimir mensagem de erro
            print(f"‚ùå Erro na resposta da API. Estado: {response.status_code}")
            return False
            
    except requests.exceptions.ConnectionError:
        # Capturar erro de conex√£o e imprimir mensagem clara
        print("‚ùå Falha ao conectar com o Ollama. Verifique se o Ollama est√° em execu√ß√£o.")
        return False
        
    except Exception as e:
        # Capturar outros poss√≠veis erros
        print(f"‚ùå Erro inesperado: {e}")
        return False


def gerar_resposta_llm(prompt_usuario: str):
    """
    Fun√ß√£o para enviar um prompt para o modelo llava-llama3 e obter a resposta.
    
    Args:
        prompt_usuario (str): O prompt/pergunta do usu√°rio
        
    Returns:
        str: A resposta do modelo ou None em caso de erro
    """
    try:
        # Endpoint da API para gera√ß√£o de texto
        url = "http://localhost:11434/api/generate"
        
        # Payload (corpo) da requisi√ß√£o JSON
        payload = {
            "model": "llava-llama3",
            "prompt": prompt_usuario,
            "stream": False
        }
        
        # Fazer requisi√ß√£o POST para o endpoint
        print("ü§ñ Gerando resposta... (isso pode levar alguns segundos)")
        response = requests.post(url, json=payload, timeout=120)
        
        # Verificar se a resposta teve status 200 (sucesso)
        if response.status_code == 200:
            # Extrair o JSON da resposta
            data = response.json()
            
            # Extrair o campo "response" do JSON
            resposta_ia = data.get('response', '')
            
            if resposta_ia:
                return resposta_ia
            else:
                print("‚ùå Resposta vazia recebida do modelo.")
                return None
                
        else:
            # Se o status for diferente de 200, imprimir mensagem de erro
            print(f"‚ùå Erro na resposta da API. Estado: {response.status_code}")
        print(f"Detalhes: {response.text}")
            return None
            
    except requests.exceptions.ConnectionError:
        # Capturar erro de conex√£o
        print("‚ùå Falha ao conectar com o Ollama. Verifique se o Ollama est√° em execu√ß√£o.")
        return None
        
    except requests.exceptions.Timeout:
        # Capturar erro de tempo limite
        print("‚ùå Tempo limite na requisi√ß√£o. O modelo pode estar demorando para responder.")
        return None
        
    except json.JSONDecodeError:
        # Capturar erro de decodifica√ß√£o JSON
        print("‚ùå Erro ao decodificar a resposta JSON da API.")
        return None
        
    except Exception as e:
        # Capturar outros poss√≠veis erros
        print(f"‚ùå Erro inesperado: {e}")
        return None


if __name__ == "__main__":
    # Configurar argparse para capturar argumentos da linha de comando
    parser = argparse.ArgumentParser(description="Conversar com o modelo llava-llama3 via Ollama")
    parser.add_argument("prompt", nargs="?", help="O prompt/pergunta para enviar ao modelo")
    parser.add_argument("--testar", action="store_true", help="Apenas testar a conex√£o com o Ollama")
    
    args = parser.parse_args()
    
    # Se a flag --testar foi usada, apenas verificar conex√£o
    if args.testar:
        verificar_modelos_ollama()
    elif args.prompt:
        # Se um prompt foi fornecido, gerar resposta
        print(f"üìù Prompt: {args.prompt}")
        print("-" * 50)
        
        resposta = gerar_resposta_llm(args.prompt)
        
        if resposta:
            print("ü§ñ Resposta da IA:")
            print(resposta)
        else:
            print("‚ùå N√£o foi poss√≠vel obter uma resposta da IA.")
    else:
        # Se nenhum argumento foi fornecido, mostrar ajuda
        print("‚ùå Por favor, forne√ßa um prompt para a IA.")
        print("Exemplo: python3.13 main.py \"Qual o sentido da vida?\"")
        print("Para testar a conex√£o: python3.13 main.py --testar")
        parser.print_help()